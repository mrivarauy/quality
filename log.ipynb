{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "log",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyNfcxyKWZjmNCUTeQvOOx+O",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/lbal-biomat/quality/blob/main/log.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DVhCv4ETdIEy"
      },
      "source": [
        "El paper en el que me guío es:\n",
        "\n",
        "Nicholls, S. M., Quick, J. C., Tang, S., & Loman, N. J. (2019). Ultra-deep, long-read nanopore sequencing of mock microbial community standards. Gigascience, 8(5), giz043. https://academic.oup.com/gigascience/article/8/5/giz043/5486468\n",
        "\n",
        "La idea es seguir los pasos del paper y ver que todo es reproducible, luego ensamblar con flye los últimos datos que publicaron (R10.3) y hacer el polish con los protocolos más recomendados, todos usando la calidad en algún punto\n",
        "\n",
        "- Flye + racon x1\n",
        "- Flye + racon x1 + medaka\n",
        "- Flye + racon x1 + homopolish\n",
        "- Flye + racon x1 + medaka + homopolish\n",
        "- Flye + racon x2\n",
        "- Flye + racon x2 + medaka\n",
        "- Flye + racon x2 + homopolish\n",
        "- Flye + racon x2 + medaka + homopolish\n",
        "- Flye + racon x4\n",
        "- Flye + racon x4 + homopolish\n",
        "- Flye + MarginPolish + helen\n",
        "- Flye + MarginPolish + homopolish\n",
        "- Flye + MarginPolish + helen + homopolish\n",
        "\n",
        "racon y marginpolish/helen usan información de calidad por base.\n",
        "\n",
        " <br/>\n",
        "\n",
        "**Instalación de programas**\n",
        "\n",
        "Instalé las versiones correspondientes al paper con conda:\n",
        "\n",
        "```\n",
        "conda create --prefix ./env_paper python=3.6 pilon=1.23 racon=1.3.2 minimap2=2.14 kraken2 checkm-genome quast=5.0.2\n",
        "```\n",
        "`wtdbg2` no estaba disponible en la versión 2.2, por lo que bajé los ejecutables de https://github.com/ruanjue/wtdbg2/releases/download/v2.2/wtdbg-2.2_x64_linux.tar.gz\n",
        "\n",
        "`medaka` tampoco estaba disponible en la versión 0.5.0, por lo que lo instalé desde la fuente en un environment aparte.\n",
        "Para eso descargue los archivos de https://pypi.org/project/medaka/0.5.0/#files. La versión correcta de medaka es importante porque la nueva no tiene el modelo correspondiente al basecaller que usaron en el paper.\n",
        "\n",
        "Antes de hacer make install a medaka hay que cambiar algunas dependencias del archivo `requirements.txt`:\n",
        "- numpy==1.19.2 en lugar de numpy, y pasar esa linea arriba de la de tensorflow\n",
        "- sacar la version a tensorflow (la que pide, 1.x.x, ya no está disponible como libreria de python)\n",
        "\n",
        " <br/>\n",
        "\n",
        "Instalé las últimas versiones de los programas en otro environment:\n",
        "\n",
        "```\n",
        "conda create --prefix ./env_nuevos flye=2.8.2 racon=1.4.13 medaka=1.2.1 minimap2=2.17 homopolish=0.0.1\n",
        "```\n",
        "homopolish tiene problemas de compatibilidad con todo así que lo puse en un env aparte:\n",
        "\n",
        "```\n",
        "conda create --prefix ./hmp homopolish \n",
        "```\n",
        "\n",
        " <br/>\n",
        "\n",
        "**Datos y descarga**\n",
        "\n",
        "Zymo Community Standards 2 (Even) Batch ZRC190633:\n",
        "10 species (5 Gram-positive, 3 Gram-negative, 2 yeast): the bacteria are present at 12% and yeast at 2% (by genomic DNA)\n",
        "\n",
        "Zymo Community Standards 2 (Log/Staggered) Batch ZRC190842\n",
        "10 species (5 Gram-positive, 3 Gram-negative, 2 yeast) ranging from 10^2 - 10^8 genomic DNA abundance (total input 5 x 10^8 cells)\n",
        "\n",
        "| Community | # of Spots    | # of Bases | Size |\n",
        "| ----------|:-------------:| ----------:|-----:|\n",
        "| Even      | 3,491,390     | 14.4G      | 12Gb | \n",
        "| Log       | \t3,667,480   |   16.5G    | 13.7Gb |\n",
        "\n",
        "\n",
        "\t\t  \t      \n",
        "\n",
        "\n",
        "  \t      \n",
        "Esos son los datos del poro 9.41 secuenciados con la plataforma GridION (para el pipeline del paper). Descargué los fastQ de https://github.com/LomanLab/mockcommunity.\n",
        "\n",
        "En el paper también usan los datos secuenciados con PromethION. Estos son muy pesados porque tienen mucha profundidad. En principio no los voy a usar.\n",
        "\n",
        "También están los datos de señal, en principio no los descargué, para seguir los pasos del paper estoy trabajando con los datos tal cual los de ellos.\n",
        "\n",
        "Descargué los datos de poro R10.3 de https://nanopore.s3.climb.ac.uk/mock/Zymo-GridION-EVEN-3Peaks-R103-merged.fq.gz. Son pocos reads (~1.16M), cuando tenga acceso a torito voy a ver de hacer una prueba con los datos que tienen publicados de poro R10 que son muchos mas reads (6.74M), disponibles en https://nanopore.s3.climb.ac.uk/mock/Zymo-GridION-Even-3Peaks-Native-R10_hac_meth.fq.gz.\n",
        "\n",
        "Están publicados los assemblies de spades con reads de illumina: https://github.com/LomanLab/mockcommunity/blob/master/analysis/illumina/spades/Zymo-Isolates-SPAdes-Illumina.fasta.gz (aún no los bajé, no estoy segura de necesitarlos).\n",
        "\n",
        "Descargué los reads de la secuenciación illumina de metagenome de la comunidad even para el polishing con pilon del SRA: ERR2984773.\n",
        "\n",
        "Me falta descargar la base de datos de microbios de kraken2:\n",
        "\n",
        "```\n",
        "wget -c https://refdb.s3.climb.ac.uk/kraken2-microbial/hash.k2d\n",
        "wget https://refdb.s3.climb.ac.uk/kraken2-microbial/opts.k2d\n",
        "wget https://refdb.s3.climb.ac.uk/kraken2-microbial/taxo.k2d\n",
        "```\n",
        "\n",
        "**Notas**\n",
        "\n",
        "Racon no anda bien con los reads comprimidos, es mejor dárselos sin comprimir. \n",
        "\n",
        "minimap2 y racon no se pueden usar con nohup porque la salida estándar va a parar al archivo final. Se puede poner el comando dentro de un script y llamar al script con nohup.\n",
        "\n",
        "El primer intento de correr racon me dio el siguiente error:\n",
        "```\n",
        "[racon::Polisher::initialize] error: empty overlap set!\n",
        "```\n",
        "Hay que borrar el newline al final del archivo .paf de minimap2, y voila!\n",
        "\n",
        "La versión 0.5.0 de medaka tiene problemas de incompatibilidad con la nueva versión de tensorflow. \n",
        "Tuve que reemplazar `tf` por `tf.compat.v1` en varios lugares de varios scripts (todos los que iban dando error), esos archivos fueron:\n",
        "- /media3/lucia/medaka-0.5.0/medaka-0.5.0/venv/lib/python3.8/site-packages/keras/backend/tensorflow_backend.py\n",
        "- /media3/lucia/medaka-0.5.0/medaka-0.5.0/venv/lib/python3.8/site-packages/medaka-0.5.0-py3.8-linux-x86_64.egg/medaka/inference.py\n",
        "\n",
        "En el archivo /media3/lucia/medaka-0.5.0/medaka-0.5.0/venv/lib/python3.8/site-packages/medaka-0.5.0-py3.8-linux-x86_64.egg/medaka/common.py tuve que cambiar, en la linea 443, `raise StopIteration` por `return`. Esto es porque en la última versión de python cambió cómo se usan los generators.\n",
        "\n",
        "Con esos cambios pude correr el programa.\n",
        "\n",
        "Estoy teniendo problemas para correr homopolish"
      ]
    }
  ]
}